{
  
    
        "post0": {
            "title": "The Markovian Sommelier",
            "content": ". &quot;Rich Tannins.&quot; &quot;Peppery finish.&quot; &quot;Afternotes of loamy soil.&quot; . Who writes wine descriptions, anyways? Wine reviews are practically a genre of their own, with a specific vocabulary and its own set of phrases and that I basically never see in any other context. . In this projet we will make a very simple model that randomly generates new wine reviews. I will walk through each step in designing the model and implementing it! . Defining the model . The model we will be using is a very simple Markov chain model. First, we model each wine review as a sequence of word pairs (i.e. bigrams). Then, we create new reviews by chaining together word pairs using a single rule which is used to generate the next word given the preceding word as input. We simply look through a dataset of real wine reviews and find all occurences of the preceding word, then randomly pick one of them and use whatever word followed it in that context. . Here&#39;s the algorithm for generating the n-th word $w_n$ given the preceding word $w_{n-1}$ and a dataset $D$: . Algorithm $g(w_n | w_{n-1}, D)$: . Find $O = {o_1, o_2, dots, o_m }$, the set of all $m$ occurences of $w_{n-1}$ in $D$ | Randomly choose an occurence $o_k in O$ | Return the word immediately following $o_k$ in its original context | . Because the generation of each word depends only on the previous word, it is completely independent of all the other preceding words in the description so far. In other words, $P(w_n | w_{n-1}) = P(w_n | w_{n-1}, w_{n-2}, dots, w_{1})$ This means that our model is a Markovian process. The transition probabilities between bigrams are empirically determined from our corpus. . Of course this is probably not going to be a great model, since it does not consider any of the context besides the immediately preceding word. But it can still give surprisingly good results, as it lets us capture many of the common two-word phrases which define the genre of wine reviews. . Now let&#39;s take a look at implementing this model. . Loading the Data . Luckily, someone has already gone through the effort of creating a dataset of more than 280,000 real wine descriptions! These were scraped from Wine Enthusiast and the dataset is hosted on Kaggle. The data have been downloaded and placed in the ./data folder. The data are split into two files. . import pandas as pd import numpy as np from collections import Counter, defaultdict import spacy # first load data data1 = pd.read_csv(&#39;./data/winemag-data-130k-v2.csv&#39;) data2 = pd.read_csv(&#39;./data/winemag-data_first150k.csv&#39;) print(data1.shape) print(data2.shape) . (129971, 14) (150930, 11) . Let&#39;s take a quick look at the datasets: . data1.head(1) . Unnamed: 0 country description designation points price province region_1 region_2 taster_name taster_twitter_handle title variety winery . 0 0 | Italy | Aromas include tropical fruit, broom, brimston... | Vulkà Bianco | 87 | NaN | Sicily &amp; Sardinia | Etna | NaN | Kerin O’Keefe | @kerinokeefe | Nicosia 2013 Vulkà Bianco (Etna) | White Blend | Nicosia | . data2.head(1) . Unnamed: 0 country description designation points price province region_1 region_2 variety winery . 0 0 | US | This tremendous 100% varietal wine hails from ... | Martha&#39;s Vineyard | 96 | 235.0 | California | Napa Valley | Napa | Cabernet Sauvignon | Heitz | . For this model, we are only interested in the descriptions, so let&#39;s pull those out and combine all the descriptions from both files: . descriptions = list(data1[&quot;description&quot;].values) + list(data2[&quot;description&quot;].values) # strip any leading or trailing whitespace if any descriptions = [string.strip() for string in descriptions] print(&quot;Total number of descriptions: &quot;, len(descriptions)) . Total number of descriptions: 280901 . Let&#39;s take a look at a few examples: . for item in np.random.choice(descriptions, size = 3): print(item, &quot; n&quot;) . Sweet mocha and coffee notes overwhelm the bouquet of this Pinot, with red raspberry and cherry skin notes providing support. Lively acidity and a satiny texture fill the mouth, while white pepper spice lingers on the finish. Hints of nail polish and flavors of hard citrus candy, with grainy honey and sugar. This is not a shy Riesling; it&#39;s intense, rich with peach and apricot, and pushed just a bit too far for some tastes. Produced by the owners of Châteauneuf-du-Pape estate Château Mont-Redon, this is a full and fruity wine. It has a good balance between acidity and red berry fruits that give a rich character. Packed with flavor, it&#39;s ready to drink. . Preprocessing the Data . Now we need to process the data to get ready for our model. But what is the best way to do this? . Data Structure . First we need to choose the data structure we will use. At its heart, our model relies on consectutive word pairs. So we could parse our dataset into a list of all word pairs, and then generate by filtering the list and randomly choosing. . However, we know that many of the word pairs will appear quite frequently! If we just parse into a list of all word pairs, we might have 100 identical entries on our list for &quot;rich tannins.&quot; We can instead count how times a word pair occurs, and keep track of the counts of all the tokens. When it comes time to sample the next word, we can simply use probabilities proportional to the counts instead of uniformly sampling! This will let us generate words without having to process the entire set of all the token pairs in our entire dataset. . In python, we will implement this as a dictionary, where each key is a token. I&#39;ll call this our vocabulary. The corresponding values are dictionaries themselves containing counts of all the tokens that followed. . Tokenizing . Each descriptions in the dataset is a single string. We need to divide the descriptions into their individual words, so we can count the word pairs. This process is called tokenization, where we divide the input into a set of tokens. . Rather than doing this from scratch, we will use a pre-made tokenizer from Spacy. The advantage of this is that the pre-made tokenizer is smart enough to handle things like puncuation. . %%time # use pre-made tokenizer from spacy nlp = spacy.load(&quot;en_core_web_sm&quot;) # a dictionary will be used to hold the vocabulary # each item in the vocabulary will have a counter to track which words follow it pair_freq = defaultdict(Counter) # make a special end of sentence token end_token = &quot;END_TOKEN&quot; # process all the descriptions # disabling unneeded components in the pipeline to speed it up for description in nlp.pipe(descriptions, disable=[&quot;tagger&quot;, &quot;parser&quot;, &quot;ner&quot;]): # for each token, update the counts of the following word for token in description: # get the following token try: neighbor = token.nbor().text except IndexError: neighbor = end_token pair_freq[token.text][neighbor] += 1 vocab = list(pair_freq.keys()) print(&quot;Total number of words:&quot;, len(vocab)) . Total number of words: 45481 CPU times: user 1min 25s, sys: 404 ms, total: 1min 26s Wall time: 1min 27s . import json with open(&#39;robosomm_data.json&#39;, &#39;w&#39;) as fp: json.dump(pair_freq, fp) . import pickle with open(&#39;robosomm_data.pickle&#39;, &#39;wb&#39;) as handle: pickle.dump(pair_freq, handle) . Our vocabulary consists of more than 45,000 unique words! . Let&#39;s look at some random examples of word pairs: . for token1 in np.random.choice(vocab, size = 10): all_following = list(pair_freq[token1].keys()) token2 = np.random.choice(all_following) print(token1, token2) . dottings of ripper , colada , assemblng quite blackberry clusters gallo salsa Barefoot sparkling sections that Carpoli has sauvage wildness . Implementing the model . First, we implement our function to generate the next word. Because we preprocessed the data in a smart way, this is actually very simple! . def gen_next_word(word): &quot;&quot;&quot;Generate the next word given the preceding word&quot;&quot;&quot; # Get the counter for the following words all_following = pair_freq[word] # Get the words themselves, and corresponding counts following_words = list(all_following.keys()) counts = np.array(list(all_following.values())) # Randomly sample the next word weights = counts / np.sum(counts) return np.random.choice(following_words, p = weights) . Now to generate a description from scratch, we just use a loop to continuously generate the next word! The loop stops when we either hit the special end-os-sentence token, or when we reach a maximum description length. . def generate_description(prompt): &quot;&quot;&quot;Generate a wine descriptions given a prompt&quot;&quot;&quot; prompt_doc = nlp(prompt) # set up the while loop current_text = prompt last_word = prompt_doc[-1].text not_end_token = True max_desc_length = 100 c = 0 while not_end_token and c &lt; max_desc_length: next_word = gen_next_word(last_word) if next_word == end_token: not_end_token = False else: current_text += &quot; &quot;+next_word last_word = next_word c += 1 return current_text . Trying it out! . Now we can generate our own wine reviews! Let&#39;s look at a few examples: . generate_description(&quot;A fruity merlot, with a smoky&quot;) . &#34;A fruity merlot, with a smoky oak . The black tea and toasty oak , apricot , allied to the next six years of lively , it &#39;s an apéritif wine very tight and soft , it too extracted Malbec . Best now . Now–2014 .&#34; . generate_description(&quot;A full bodied cabernet&quot;) . &#34;A full bodied cabernet sauvignon . It has honey , it &#39;s a delicious , and berry fruits and rich future .&#34; . generate_description(&quot;Spicy&quot;) . &#39;Spicy cinnamon , it would pair with hearty mouthful of Pinot they are tougher , currants , cherries lead to the finish .&#39; . generate_description(&quot;This wine is terrible&quot;) . &#39;This wine is terrible flaws here . In the black fruit . It feels tight tannins , luscious and fresh and sophisticated notes , this wine offers aromas emerge with ample cherry flavors . The finish is very impressive is a bit of cherry , which offers a shame to soften . In the ripe and Mourvèdre , with suggesting wet cement , juicy and bitter , this 100 % Syrah with just yearning to say that will put in French oak flavors are certified - dimensional in the perfumes , packed with mixed with mature fruit and minerality and a final indication of&#39; . Conclusion . There we have it! A (very rudimentary) text generation model! . The descriptions certainly aren&#39;t great - I don&#39;t think any human would be fooled! However, given how rudimentary our model is, the results are surprisingly good. The sentences are mostly coherent, and they also do well at capturing the vocabulary and phrases distinctive of the wine description genre! This shows how even the simplest model can &quot;learn&quot; features distinctive of the dataset it was trained on. . Of course we could improve on this model by using 3-grams or 4-grams instead of bigrams, which would let us capture more context. Or, we could use NLP methods that are much better than Markov chains! Recurrent neural networks, transformers, etc... Maybe we&#39;ll look at those in a future notebook. . In the meantime, enjoy this Markovian Sommelier! .",
            "url": "https://jacobrosenth.al/posts/2020/10/15/wine-review-generation.html",
            "relUrl": "/2020/10/15/wine-review-generation.html",
            "date": " • Oct 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jacobrosenth.al/posts/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jacobrosenth.al/posts/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Building a Camera Obscura",
            "content": "About . For this project, I worked with my partner Lucie Gillet to build a camera obscura from scratch. We worked on this as part of an assignment for MIT 6.869: Advances in Computer Vision (Fall 2019) taught by Professors Bill Freeman, Antonio Torralba, and Philip Isola. Course website here. . What is a camera obscura? . A camera obscura is a dark chamber with a single, tiny hole (the pinhole) in one side that lets light in. Rays of light from the outside enter the pinhole, and hit the wall opposite the pinhole, forming a projected image: . . The image is inverted top-bottom and right-left because it is projected through the pinhole. . The image is projected inside the camera - so how do we see it? Some camera obscuras are built as structures large enough to go inside and look at the projection on a screen. However, in our case we created a small camera from a cardboard box and then used the digital cameras on our phones to capture the projected images. . How to build the pinhole camera . Instructions copied from the assignment . Find a cardboard box. | Make a tiny hole (~= 5 mm) in the center of one of the faces. This will be your pinhole. | Make a second hole, just big enough for your camera. If you’re using a professional camera, make a hole just wide enough for your lens to fit. If you’re using a phone, make a small hole such that you can press your phone to the box, and the camera will not be obstructed. | Cover the inside walls with black paper to reduce inter-reflections. | Place a sheet (or more) of white paper on the wall opposite your pinhole. This white paper will be the screen onto which your image is projected. | (Optional) You may also want to use dark tape along the edges and corners of your box - where two separate pieces of cardboard meet, there may a small slit, which can let light in! (we ended up wrapping the whole camera in foil to solve this problem) | . Here’s what our camera looked like from the outside: . . And here is a side view of the inside, showing the arrangement of the pinholes and the screen: . . Taking Pictures . Since a camera obscura lets in so little light (to keep the image sharp), we made sure to get as much light as possible by taking our pictures outside, in the daytime, on a sunny day. . In our first attempt, we learned the camera must remain perfectly still - if it moves at all, the images will be blurry: . . After some trial and error, we managed to capture some good quality pictures: . . . Conclusion . Making a camera obscura is a fun afternoon project, and you can get surprisingly good results! Make sure to block out all light from entering the box through holes or cracks, and try it out on a sunny day! .",
            "url": "https://jacobrosenth.al/posts/2019/09/26/camera-obscura.html",
            "relUrl": "/2019/09/26/camera-obscura.html",
            "date": " • Sep 26, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My passion is developing computational tools to learn useful information from vast troves of biomedical data (images, text, genomics, etc.) and finding ways to apply them in the realms of clinical medicine and basic research in order to tangibly improve patient care and advance our fundamental knowledge of the biological processes underlying disease. . My work lies at the intersection of medicine and machine learning, aiming to bridge the gap between these two worlds. I am interested in both the technical aspects (data engineering, software development, machine learning methods, etc.) as well as the biomedical aspects (clinical workflows, diagnostics, cancer biology, etc.). . I currently work as a data scientist at Dana-Farber Cancer Institute in Boston. Before that, I studied data science at Harvard University. Before that, I studied biology at Oberlin College. Before that, I grew up in Seattle. . Hobbies include running, playing chess, exploring new breweries with friends, taking care of my houseplants, and drinking copious amounts of coffee. .",
          "url": "https://jacobrosenth.al/posts/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jacobrosenth.al/posts/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}